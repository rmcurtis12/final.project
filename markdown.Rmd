---
title: "Final Project"
author: "Takashi Takizawa"
date: "2017/4/8"
output: html_document
---

#Table of contents
1. Data cleaning & preparation
1.1 Step logit
1.2 Random forest on variables ID'd by step logit
2. KNN model
3. SVM model
4. ANN model
4. Random Forest
5. ANN model
6. Model Accuracy
7. Create Ensemble Model
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985


#1. Data cleaning & preparation 

Import library
```{r}
rm(list = ls())
#install.packages("party")
# install.packages("beepr")
# install.packages("caret")
# install.packages("ada")
#install.packages("e1071")
#install.packages("multcomp")
#install.packages("party")

library(multcomp)
library(party)
library(plyr)
library(dplyr)
library(class)
library(gmodels)
library(stats)
library(magrittr)
library(beepr)
library(caret)
library(kernlab)
library(neuralnet)
library(randomForest)
library(ada)
library(C50)

```

Aux functions
```{r}

midbeep <- function() {beepr::beep("coin")}
endbeep <- function() {beepr::beep("complete")}
```

Data cleaning
```{r setup, include=FALSE}
speed <- read.csv("Speed Dating Data.csv")
speed$condtn <- as.factor(speed$condtn)
speed$gender <- as.factor(speed$gender)
speed$match <- as.factor(speed$match)
speed$field_cd <- as.factor(speed$field_cd)
levels(speed$field_cd) <- c("Law","Math","SocScie/Psych", "MedSci", "Engineering", "English", "History", "Business", "Education", "Bio","SocialWork","Undergrad", "PoliSci", "Film","FineArts","Lang","Architecture","Other")
speed$race <- as.factor(speed$race)
speed$goal <- as.factor(speed$goal)
levels(speed$goal) <- c("FunNightOut", "MeetNewPpl", "GetADate","SRSRelationship", "ToSayIDidIt","Other")

speed$date <- as.factor(speed$date)
#Don't convert to date as character
#levels(speed$date) <- c("SVRL/Week","2/Week","1/Week","2/Month", "1/Month", "SVRL/Year", "AlmostNever")

speed$go_out <- as.factor(speed$go_out)
levels(speed$go_out) <- c("SVRL/Week","2/Week","1/Week","2/Month", "1/Month", "SVRL/Year", "AlmostNever")
speed$career_c <-as.factor(speed$career_c)
levels(speed$career_c) <- c("Lawyer","Academic/Research","Psychologist","DocMed", "Engineer", "Entertainment", "Banking/Consulting", "RealEstate","IntlAffairs","Undecided","SocialWork","SpeechPath","Politics", "ProSports", "Other", "Journalism", "Architecture")
speed$race_o <-as.factor(speed$race_o) 
#speed$dec_o <- as.factor(speed$dec_o)
#speed$samerace <- as.factor(speed$samerace)

sd2 <- speed
sd2 <- sd2[ , -1] #IID  
sd2 <- sd2[, -1] #ID  
sd2 <- sd2[, -2] #IDG
sd2 <- sd2[, -3] #Wave
sd2 <- sd2[, -3] #Round
sd2 <- sd2[, -3] #Position
sd2 <- sd2[, -3] #Postion1
sd2 <- sd2[, -4] #Partner 
sd2 <- sd2[, -4] #PID
sd2 <- sd2[, -26]#Field
sd2 <- sd2[, -(27:29)]#Academics
sd2 <- sd2[,-(30:32)]#Socioeconomic 
sd2 <- sd2[,-33]#Career
sd2 <- sd2[,-(59:64)]#What others look for
sd2 <- sd2[,-(70:74)]#Others perception
sd2 <- sd2[,-(81:92)]#Data gathered after intitial
sd2 <- sd2[,(1:79)]
sd2 <- sd2[,-(70:79)] #Removes Post First Date
sd2 <- sd2[,-52]#exclude expnum

# sd2$mn_sat <- speed$mn_sat
# sd2$mn_sat[sd2$mn_sat==""]<-NA

sdrandom <- sd2[sample(nrow(sd2), nrow(sd2)),] #Get a random sample since the data is organized by participant

sdclean <- na.omit(sdrandom) #Remove rows with NA values to create a "clean" set
```


Create new variables to show gaps between objects and partners
```{r}
normalize<-function(x){
  return((x-min(x))/(max(x)-min(x)))}

#age difference
# sdclean$age_dif<-sdclean$age - sdclean$age_o

# #intelligence difference between sat score and partner's preference
# x<-data.frame(lapply(sdclean[18], normalize))
# sdclean$mn_sat<-as.numeric(sdclean$mn_sat)
# y<-as.data.frame(lapply(sdclean[69], normalize))
# z<- x-y

#partner's preference and actual rating
sdclean$att_dif<-sdclean$pf_o_att - sdclean$attr1_1
sdclean$sinc_dif<-sdclean$pf_o_sin - sdclean$sinc1_1
sdclean$intel_dif<-sdclean$pf_o_int - sdclean$intel1_1
sdclean$fun_dif<-sdclean$pf_o_fun - sdclean$fun1_1
sdclean$amb_dif<-sdclean$pf_o_amb - sdclean$amb1_1
sdclean$shar_dif<-sdclean$pf_o_sha - sdclean$shar1_1

```

Normarize variables
```{r}
#age difference
# a<-data.frame(lapply(sdclean[69], normalize))
# sdclean$age_dif_norm<-a$age_dif

#intelligence difference
# b<-as.data.frame(lapply(z[1], normalize))
# sdclean$intel_dif<-b$intel_o

#date frequency
# sdclean$date<-as.numeric(sdclean$date)
# c<-data.frame(lapply(sdclean[31], normalize))
# sdclean$date_norm<-c$date

#partner's preference and actual rating
d<-data.frame(lapply(sdclean[69], normalize))
sdclean$att_dif_norm<-d$att_dif

e<-data.frame(lapply(sdclean[70], normalize))
sdclean$sinc_dif_norm<-e$sinc_dif

f<-data.frame(lapply(sdclean[71], normalize))
sdclean$intel_dif_norm<-f$intel_dif

g<-data.frame(lapply(sdclean[72], normalize))
sdclean$fun_dif_norm<-g$fun_dif

h<-data.frame(lapply(sdclean[73], normalize))
sdclean$amb_dif_norm<-h$amb_dif

i<-data.frame(lapply(sdclean[74], normalize))
sdclean$shar_dif_norm<-i$shar_dif

#rating of partners by objects (inter-correlation)
# j<-data.frame(lapply(sdclean[5], normalize))
# sdclean$int_corr_norm<-j$int_corr


```

Delete unnecessary columns
```{r}
sdfinal <- sdclean[,c(4,75:80)]
str(sdfinal)
```

Create train & test data
```{r}
##Make train and test data
train_data<-sdfinal[1:4500,]
test_data<-sdfinal[4501:6499,]

#Make labels for train and test data
train_data_label<-sdfinal[1:4500,1]
test_data_label<-sdfinal[4501:6499,1]

```

#1.1 Step Logit to identify significant prediction variables
```{r}
#Working with sdclean data, not sdfinal
##Make train and test data
train_data_sdclean<-sdclean[1:4500,]
test_data_sdclean<-sdclean[4501:6499,]

#str(sdclean)

#Make labels for train and test data
train_label_sdclean<-sdclean[1:4500,1]
test_label_sdclean<-sdclean[4501:6499,1]

log_step_f <- colnames(sdclean) %>% 
    {paste(.[! . %in% "match"], collapse = " + ")} %>% 
    paste("match ~ ", .) %>% 
    as.formula()

log_model_step <- glm(match ~ 1, data = train_data_sdclean, family = binomial)
log_model_step <- step(log_model_step, scope = (log_step_f), direction = "forward")
  
summary(log_model_step)
save(log_model_step,file = "log_model_step.txt")

sig_var = c("prob_o","gender","pf_o_sha","met_o","race","exphappy","sports","attr1_1","pf_o_sin","go_out","fun_o","attr_o","age_o","int_corr","yoga","shar2_1","shopping")


```

#1.2 Random forest on significant variables
```{r}
#create the formula for random forest "match ~ blah + blah + blah..."
ran_for_f <- sig_var %>% 
    {paste(.[! . %in% "match"], collapse = " + ")} %>% 
    paste("match ~ ", .) %>% 
    as.formula()

#run the RANdom FORest on the SIGnificant VARiables
ran_for_sig_var <- randomForest(formula = ran_for_f, data = train_data_sdclean, importance=TRUE)

plot(ran_for_sig_var)

summary(ran_for_sig_var)

varImpPlot(ran_for_sig_var,
           sort = T,
           main="Variable Importance")



```


#2. KNN model
```{r}
library(class)

#Delete target variable from train data
train_data_knn<-train_data[-1]
test_data_knn<-test_data[-1]

#Develop KNN Model
<<<<<<< HEAD
knn_data_pred<-knn(train=data_train_knn, test=data_test_knn, cl=data_train_label, k=44)

#Inspect model accuracy
#library(gmodels)
#CrossTable(x=data_test_label,y=knn_data_pred,prop.chisq = FALSE)
=======
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985

run_all_knn_models = F

if (run_all_knn_models) {
  knn_model_one <- knn(train=train_data_knn, test=test_data_knn, cl=train_data_label, k=5)
  knn_model_two <- knn(train=train_data_knn, test=test_data_knn, cl=train_data_label, k=10)
  
  midbeep()
  endbeep()
  save(knn_model_one, file = "knn_model_one.txt")
  save(knn_model_two, file = "knn_model_two.txt")
#  save(knn_model_train, file = "knn_model_train.txt") 
} else {
  load(file = "knn_model_one.txt")
  # load(file = "knn_model_train.txt") 
  load(file = "knn_model_two.txt")
}
```


#3. SVM model 
```{r}
<<<<<<< HEAD
library(kernlab)

#Develop SVM model
data_classifier <- ksvm(match ~ ., data = data_train, kernel = "vanilladot")
svm_data_pred <- predict(data_classifier, data_test)
svm_data_pred <- ifelse(svm_data_pred>0.5, 1, 0)

#Inspect model accuracy
#table(svm_data_pred, data_test$match)
#agreement <- svm_data_pred == data_test$match
#table(agreement)
#prop.table(table(agreement))

#89.9% accuracy in the previous model run
=======
run_all_svm_models = F

if (run_all_svm_models) {
  svm_model_one <- ksvm(match ~ ., data = train_data, kernel = "vanilladot")
  svm_model_two <- ksvm(match ~ ., data = train_data, kernel = "rbfdot")
  midbeep()
  # svm_model_train decided to error out on us
  svm_model_train <- caret::train(match ~ ., data = train_data, 
                                  method = "svmRadialSigma", 
                                  metric = "Kappa")#, 
                                  # trcontrol = fitControl)
  endbeep()
  save(svm_model_one, file = "svm_model_one.txt")
  save(svm_model_two, file = "svm_model_two.txt")
  save(svm_model_train, file = "svm_model_train.txt") 
} else {
  load(file = "svm_model_one.txt")
  # load(file = "svm_model_train.txt") 
  load(file = "svm_model_two.txt")
}
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985
```

#4. Random Forest
```{r}
<<<<<<< HEAD
library(neuralnet)
=======
run_all_tree_models = F

if (run_all_tree_models) {
  # Basic Classification Tree
  tree_model <- ctree(match ~ ., data = train_data, 
                      control = ctree_control(mincriterion = .99))
  tree_random_forest <- randomForest(match~., data = train_data)
  midbeep()
  # Ada Tree
  tree_ada <- ada(match~., data = train_data)
  save(tree_model, file = "tree_model.txt")
  save(tree_random_forest, file = "tree_random_forest.txt")
  save(tree_ada, file = "tree_ada.txt")
} else {
  load(file = "tree_model.txt")
  load(file = "tree_random_forest.txt")
  load(file = "tree_ada.txt")
}
```
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985

#5. ANN model
```{r}
data_n <- model.matrix(~. + 0, data = sdfinal)
nnformula <- colnames(data_n) %>% {as.formula(paste("match ~", paste(.[!. %in% "match"], collapse = " + ")))}
train_data_indicies <- sample(1:nrow(sdfinal), 
                              replace = F, 
                              size = floor(nrow(sdfinal) * 0.8)) 
data_n_train <- data_n[1:nrow(data_n) %in% train_data_indicies,]
data_n_test <- data_n[!(1:nrow(data_n) %in% train_data_indicies),]

run_all_nn_models = T

  if (run_all_nn_models) {
    # The neural net will not work with the standard telemarketing data
    neural_model <- neuralnet::neuralnet(formula = as.factor(match), data = train_data)
    midbeep()
    neural_model_list <- list()
    
    neural_seq <- seq(from = 6, to = 20, by = 4)
    nn_models_big <- lapply(neural_seq, function(i) {
      cat("Working on:", i, "\n")
      return(neuralnet::neuralnet(formula = as.factor(match), 
                                       data = train_data, 
                                       hidden = i, 
                                       stepmax = 1e05))
      midbeep()
    })

  neural_model_2 = nn_models_big[[1]]
  # All of the rest of the neural models blew up
  save(nn_models_big, file = "nn_models_big")
  save(neural_model, file = "neural_model.txt")
  save(neural_model_2, file = "neural_model_2.txt")
} else {
  load(file = "neural_model.txt")
  load(file = "neural_model_2.txt")
}
```

#6. Model Accuracy
```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}

knn_prediction_01 <- knn_model_one
knn_prediction_02 <- knn_model_two

svm_prediction_01 <- predict(svm_model_one, test_data)
svm_prediction_02 <- predict(svm_model_two, test_data)

# neuralnet_prediction <- neuralnet::compute(neural_model, telemarketing_n_test[, 1:54])$net.result %>% {ifelse(. > 0.5, "yes", "no")} %>% as.factor()
# neuralnet_prediction_2 <- neuralnet::compute(neural_model_2, telemarketing_n_test[, 1:54])$net.result %>% {ifelse(. > 0.5, "yes", "no")} %>% as.factor()

tree_prediction <- predict(tree_model, test_data)
tree_random_forest_prediction <- predict(tree_random_forest, test_data)
tree_ada_prediction <- predict(tree_ada, test_data)
```

<<<<<<< HEAD
#how did we do?
#ann_output <- CrossTable(x = data_test_label, y = ann_predicted_match, 
           prop.chisq=FALSE)
=======
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985

```{r}
a2 = accuracy(knn_prediction_01, test_data$match, "KNN ONE", TRUE)
a3 = accuracy(knn_prediction_02, test_data$match, "KNN TWO", TRUE)
a4 = accuracy(svm_prediction_01, test_data$match, "SVM with Vanilla Kernal", TRUE)
a5 = accuracy(svm_prediction_02, test_data$match, "SVM Radial Basis Kernal", TRUE)
# a6 = accuracy(neuralnet_prediction, test_data$match, "Neural Net One Hidden Node", TRUE)
# a7 = accuracy(neuralnet_prediction_2, test_data$match, "Neural Net Two Hidden Nodes", TRUE)
a8 = accuracy(tree_prediction, test_data$match, "CTree Regression", TRUE)
a9 = accuracy(tree_random_forest_prediction, test_data$match, "Random Forest Classification", TRUE)
a10 = accuracy(tree_ada_prediction, test_data$match, "Ada Boost Classification", TRUE)

acc_predictions = c(a2,a3,a4,a5,a8,a9,a10)
names = c("KNN ONE", "KNN TWO", "SVM with Vanilla Kernal","SVM Radial Basis Kernal","CTree Regression","Random Forest Classification","Ada Boost Classification")

acc_mat <- data.frame(ModelName = names, accuracy = acc_predictions) %>% print
```

```{r}
dotchart(acc_predictions, labels = names, main = "Accuracy of the models", xlab = "Accuracy")
```

#5. Model Accuracy
```{r}

accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}

print(accuracy(svm_data_pred,data_test_label,"SVM"))
print(accuracy(knn_data_pred,data_test_label,"kNN"))
print(accuracy(ann_predicted_match,data_test_label,"ANN"))

```


<<<<<<< HEAD
#6. Create Ensemble Model
=======
#7. Create Ensemble Model
>>>>>>> 486378e7f645d9942665e09b539e589c5baac985
```{r}

# Convert all of the feature data to factors....
ConvertToYesNo <- function(myprediction) {
  result <- myprediction %>% as.factor()
  levels(result) <- c("no", "yes")
  result
}

library(magrittr)
library(dplyr)

# Convert each set of predictions to factors
knn_data_pred %<>% ConvertToYesNo()
svm_data_pred %<>% ConvertToYesNo()
ann_predicted_match %<>% ConvertToYesNo()

model_combined_results <- data.frame(knn_data_pred, svm_data_pred, ann_predicted_match) %>% as.tbl()

summary(model_combined_results)
str(data_test_label)

# Create the model -> Stacking with a Ctree to find out if it helps any!
library(partykit)

stacked_model <- ctree(data_test_label ~ ., data = model_combined_results) %T>% plot
#, controls = ctree_control(mincriterion = 0.99999)

```


